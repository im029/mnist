{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST Classification\n",
    "\n",
    "The MNIST dataset of handwritten digits is the `hello world` for anyone starting with Deep Learning.  \n",
    "\n",
    "Some facts about the MNIST dataset:\n",
    "* Contains 70,000 examples of handwritten digits\n",
    "* 60,000 are training examples and 10,000 are testing examples\n",
    "* Each image is of size 28 * 28\n",
    "* Images in the dataset are grayscale \n",
    "\n",
    "For detailed info visit http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries\n",
    "\n",
    "[**Tensorflow**](https://www.tensorflow.org) for creating neural networks. Notice that tensorflow comes with the MNIST data set included. The dataset included is nicely split into testing and training sets already as we will see.\n",
    "\n",
    "[**Matplotlib**](https://matplotlib.org/) for creating graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import the mnist dataset that comes with tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# allows graph to be drawn inline and shown in output.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.cmap'] = 'Greys'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###### Now we will inspect the shapes of our training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training images: (55000, 784)\n",
      "Shape of the training labels: (55000, 10)\n",
      "=============\n",
      "Shape of the testing images: (10000, 784)\n",
      "Shape of the testing labels: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the training images: {}\".format(mnist.train.images.shape))\n",
    "print(\"Shape of the training labels: {}\".format(mnist.train.labels.shape))\n",
    "\n",
    "print(\"=============\")\n",
    "\n",
    "print(\"Shape of the testing images: {}\".format(mnist.test.images.shape))\n",
    "print(\"Shape of the testing labels: {}\".format(mnist.test.labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 55000 training examples. We will use this set for training our neural network\n",
    "\n",
    "Interesting thing to note is that each example has dimension 784. As mentioned earlier the images in MNIST dataset are 28 X 28. We can visualize that image as 2d matrix of dimension 28 X 28 (put these numbers into your calculator). This matrix can be rearranged as 784 X 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect one of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x180c2b428d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x180c2a6f2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = mnist.test.images[1]\n",
    "plt.imshow(image.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two placeholders one for input and one for output. Notice that the first dimension is `None`. This means that we are asking tensorflow to decide that dimension automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholders for input data, note that the first dimension is None.\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"Inputs\")\n",
    "# create placeholder for outputs. We have 10 possible outputs\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"Outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Neural Network Architecture\n",
    "\n",
    "We have the following architecture.\n",
    "\n",
    "2 hidden layers.\n",
    "\n",
    "1st hidden layer: Input 784 -> ReLu -> Output 400  \n",
    "2nd hidden layer: Input 400 -> ReLu -> Output 100\n",
    "\n",
    "Output Layer: Input 100 -> softmax -> Output 10\n",
    "\n",
    "The softmax outputs the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "# create a weight matrix\n",
    "W1 = tf.Variable(tf.truncated_normal(shape=[784, 400], stddev=0.1), name=\"Weights_Layer1\")\n",
    "# bias matrix\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[400]), name=\"bias_Layer1\")\n",
    "# activation function\n",
    "a1 = tf.nn.relu(tf.matmul(x, W1) + b1, name=\"RELU_1\")\n",
    "\n",
    "# Layer 2\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[400, 100], stddev=0.1), name=\"Weights_Layer2\")\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[100]), name=\"bias_Layer2\")\n",
    "a2 = tf.nn.relu(tf.matmul(a1, W2) + b2, name=\"RELU_2\")\n",
    "\n",
    "# Output Layer\n",
    "W3 = tf.Variable(tf.truncated_normal(shape=[100, 10], stddev=0.1), name=\"Weights_Output\")\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[10]), name=\"bias_Output\")\n",
    "# softmax to generate probability distribution\n",
    "y = tf.nn.softmax(tf.matmul(a2, W3) + b3, name=\"softmax_Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loss function\n",
    "The loss function is used by the optimizer. The optimizer tries to minimize the loss.\n",
    "\n",
    "We are using the cross entropy loss. Tensorflow provides builtin in cross entropy loss function we can also use that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "Optimizer is the main workhorse here. The optimizer will adjust the weights and learn the generalized function.\n",
    "\n",
    "There are various optimizers available. We will be using [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 1000 epochs taking 100 examples at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # train in minibaches of 100\n",
    "    batch_x, batch_y = mnist.train.next_batch(100)\n",
    "    # this line trains the network\n",
    "    sess.run(train_step, feed_dict={x: batch_x, y_: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check accuracy\n",
    "\n",
    "We will check predicted value against the gound truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.17%\n",
      "Loss: 0.0914\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "ac, loss = sess.run([accuracy, cross_entropy], feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "\n",
    "print(\"Accuracy: {:0.2f}%\".format(ac * 100))\n",
    "print(\"Loss: {:0.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remarks\n",
    "***\n",
    "\n",
    "We trained a Simple Neural Network for MNIST classification and achieved fairly high accuracy. However, there are few things that we left.\n",
    "\n",
    "In MNIST, there are 60000 training examples, but when we printed the shape of training set there were 55000 examples only. Where did those 5000 go?\n",
    "\n",
    "We will explore this later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
